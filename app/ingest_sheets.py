"""
Google Sheets data ingestion for Bowling RAG system
Loads data from Google Sheets, processes it, and stores in Qdrant vector database
"""

import uuid
from typing import List, Dict, Any, Optional
import numpy as np
from tqdm import tqdm
from datetime import datetime

import gspread
from google.oauth2.service_account import Credentials

from sentence_transformers import SentenceTransformer
from qdrant_client import QdrantClient
from qdrant_client.models import VectorParams, Distance, PointStruct

import os
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

from app.config import (
    SERVICE_ACCOUNT_FILE,
    SPREADSHEET_ID,
    QDRANT_HOST, QDRANT_API_KEY, COLLECTION_NAME,
    EMBEDDING_MODEL_NAME, EMBEDDING_DIM,
    SHEETS_CONFIG,
    CHUNK_SIZE, CHUNK_OVERLAP
)

def chunk_text(text: str, chunk_size: int, overlap: int) -> List[str]:
    """Split text into overlapping chunks of specified size"""
    if not text:
        return []

    text = str(text).strip()
    if len(text) <= chunk_size:
        return [text]

    chunks = []
    step = max(1, chunk_size - overlap)

    for start in range(0, len(text), step):
        piece = text[start:start + chunk_size]
        if len(piece) < 50:  # Skip very small chunks
            break
        chunks.append(piece)
        if start + chunk_size >= len(text):
            break

    return chunks

def ensure_collection(client: QdrantClient, vector_size: int):
    """Create or recreate collection if vector size changed"""
    if not client.collection_exists(COLLECTION_NAME):
        print(f"[INFO] Collection '{COLLECTION_NAME}' not found, creating new...")
        client.create_collection(
            collection_name=COLLECTION_NAME,
            vectors_config=VectorParams(size=vector_size, distance=Distance.COSINE)
        )
        print(f"[SUCCESS] Collection '{COLLECTION_NAME}' created")
    else:
        print(f"[INFO] Collection '{COLLECTION_NAME}' already exists")

def authenticate_gsheets():
    """Authenticate with Google Sheets API"""
    try:
        creds = Credentials.from_service_account_file(
            SERVICE_ACCOUNT_FILE,
            scopes=['https://www.googleapis.com/auth/spreadsheets.readonly']
        )
        client = gspread.authorize(creds)
        print("[SUCCESS] Google Sheets authentication successful")
        return client
    except Exception as e:
        print(f"[ERROR] Google Sheets authentication failed: {e}")
        raise

def load_sheet_data(gs_client: gspread.Client, sheet_name: str) -> List[Dict[str, Any]]:
    """Load data from specified Google Sheet"""
    try:
        spreadsheet = gs_client.open_by_key(SPREADSHEET_ID)
        worksheet = spreadsheet.worksheet(sheet_name)

        # Get all values
        all_values = worksheet.get_all_values()

        if not all_values:
            print(f"[WARNING] Sheet '{sheet_name}' is empty")
            return []

        # First row is headers
        headers = all_values[0]
        rows = all_values[1:]

        print(f"[INFO] Loaded {len(rows)} rows from sheet '{sheet_name}'")

        # Convert to dictionary format
        config = SHEETS_CONFIG.get(sheet_name, {})
        data = []

        for row_idx, row in enumerate(rows):
            if not any(row):  # Skip empty rows
                continue

            row_data = {}
            for col_idx, value in enumerate(row):
                if col_idx < len(headers):
                    header = headers[col_idx].lower().strip()
                    row_data[header] = value

            # Add UUID if not present
            if 'uuid' not in row_data or not row_data['uuid']:
                row_data['uuid'] = str(uuid.uuid4())

            # Validate required fields
            if not row_data.get('question') or not row_data.get('answer'):
                print(f"[WARNING] Row {row_idx + 2} missing question or answer, skipping")
                continue

            data.append(row_data)

        print(f"[SUCCESS] Processed {len(data)} valid entries from '{sheet_name}'")
        return data

    except Exception as e:
        print(f"[ERROR] Failed to load data from sheet '{sheet_name}': {e}")
        raise

def process_faq_data(faq_data: List[Dict[str, Any]]) -> List[PointStruct]:
    """Process FAQ data into Qdrant points"""
    points = []

    for item in tqdm(faq_data, desc="Processing FAQ data"):
        try:
            # Extract fields with corrected column names
            question = str(item.get('question', '')).strip()
            answer = str(item.get('answer', '')).strip()  # Fixed: was 'anwser'
            category = str(item.get('category', '')).strip()
            tags = str(item.get('tags', '')).strip()
            source = str(item.get('source', '')).strip()  # Fixed: was 'sourse'
            image_url = str(item.get('image_url', '')).strip()
            last_updated = str(item.get('last_updated', '')).strip()
            item_uuid = str(item.get('uuid', str(uuid.uuid4())))

            # Skip if no content
            if not question and not answer:
                continue

            # Create searchable text
            searchable_text = f"{question} {answer}".strip()

            # Create chunks if text is long
            if len(searchable_text) > CHUNK_SIZE:
                chunks = chunk_text(searchable_text, CHUNK_SIZE, CHUNK_OVERLAP)
            else:
                chunks = [searchable_text]

            # Create points for each chunk
            for chunk_idx, chunk in enumerate(chunks):
                # Create unique ID for this chunk
                chunk_id = f"{item_uuid}_chunk_{chunk_idx}"

                # Prepare payload
                payload = {
                    "uuid": item_uuid,
                    "chunk_id": chunk_id,
                    "question": question,
                    "answer": answer,
                    "category": category,
                    "tags": tags,
                    "source": source,
                    "image_url": image_url,
                    "last_updated": last_updated,
                    "chunk_text": chunk,
                    "chunk_index": chunk_idx,
                    "total_chunks": len(chunks),
                    "created_at": datetime.now().isoformat()
                }

                # Generate embedding for the chunk
                embedding = embedder.encode(chunk).tolist()

                # Create point
                point = PointStruct(
                    id=chunk_id,
                    vector=embedding,
                    payload=payload
                )

                points.append(point)

        except Exception as e:
            print(f"[ERROR] Failed to process item {item.get('uuid', 'unknown')}: {e}")
            continue

    print(f"[SUCCESS] Created {len(points)} points from {len(faq_data)} FAQ items")
    return points

def upload_to_qdrant(client: QdrantClient, points: List[PointStruct]):
    """Upload points to Qdrant in batches"""
    if not points:
        print("[WARNING] No points to upload")
        return

    batch_size = 100
    total_uploaded = 0

    try:
        # Clear existing data (optional - comment out if you want to keep existing data)
        print("[INFO] Clearing existing collection data...")
        client.delete_collection(COLLECTION_NAME)
        client.create_collection(
            collection_name=COLLECTION_NAME,
            vectors_config=VectorParams(size=EMBEDDING_DIM, distance=Distance.COSINE)
        )

        # Upload in batches
        for i in tqdm(range(0, len(points), batch_size), desc="Uploading to Qdrant"):
            batch = points[i:i + batch_size]
            client.upsert(collection_name=COLLECTION_NAME, points=batch)
            total_uploaded += len(batch)

        print(f"[SUCCESS] Uploaded {total_uploaded} points to Qdrant")

    except Exception as e:
        print(f"[ERROR] Failed to upload to Qdrant: {e}")
        raise

def main():
    """Main ingestion function"""
    print("üöÄ Starting Bowling RAG data ingestion...")
    print("=" * 50)

    try:
        # Initialize embedder
        global embedder
        print("[INFO] Initializing embedder...")
        embedder = SentenceTransformer(EMBEDDING_MODEL_NAME)
        print(f"[SUCCESS] Embedder initialized: {EMBEDDING_MODEL_NAME}")

        # Initialize Qdrant client
        print("[INFO] Initializing Qdrant client...")
        qdrant_client = QdrantClient(url=QDRANT_HOST, api_key=QDRANT_API_KEY)
        ensure_collection(qdrant_client, EMBEDDING_DIM)
        print("[SUCCESS] Qdrant client initialized")

        # Authenticate with Google Sheets
        print("[INFO] Authenticating with Google Sheets...")
        gs_client = authenticate_gsheets()

        # Load FAQ data
        print("[INFO] Loading FAQ data from Google Sheets...")
        faq_data = load_sheet_data(gs_client, "FAQ")

        if not faq_data:
            print("[ERROR] No data loaded from Google Sheets")
            return

        # Process data into points
        print("[INFO] Processing data into vector points...")
        points = process_faq_data(faq_data)

        # Upload to Qdrant
        print("[INFO] Uploading data to Qdrant...")
        upload_to_qdrant(qdrant_client, points)

        print("=" * 50)
        print("üéâ Data ingestion completed successfully!")
        print(f"üìä Total points uploaded: {len(points)}")
        print("üöÄ Your RAG system is ready to use!")

    except Exception as e:
        print(f"[ERROR] Data ingestion failed: {e}")
        raise

if __name__ == "__main__":
    main()

def chunk_text(text: str, chunk_size: int, overlap: int) -> List[str]:
    """–†–µ–∂–µ–º –¥–ª–∏–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –Ω–∞ –ø–µ—Ä–µ–∫—Ä—ã–≤–∞—é—â–∏–µ—Å—è –∫—É—Å–æ—á–∫–∏ —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –¥–ª–∏–Ω—ã."""
    if not text:                                         # –ï—Å–ª–∏ –ø—É—Å—Ç–æ ‚Äî –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫
        return []
    text = str(text).strip()                             # –°—Ç—Ä–∞—Ö—É–µ–º—Å—è: –ø—Ä–∏–≤–æ–¥–∏–º –∫ —Å—Ç—Ä–æ–∫–µ –∏ —É–±–∏—Ä–∞–µ–º –ø—Ä–æ–±–µ–ª—ã –ø–æ –∫—Ä–∞—è–º
    if len(text) <= chunk_size:                          # –ï—Å–ª–∏ –∫–æ—Ä–æ—Ç–∫–æ ‚Äî –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∫–∞–∫ –µ—Å—Ç—å –æ–¥–Ω–∏–º —á–∞–Ω–∫–æ–º
        return [text]
    chunks = []                                          # –°–ø–∏—Å–æ–∫ —á–∞–Ω–∫–æ–≤
    step = max(1, chunk_size - overlap)                  # –®–∞–≥ —Å–¥–≤–∏–≥–∞: –¥–ª–∏–Ω–∞ —á–∞–Ω–∫–∞ - –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ
    for start in range(0, len(text), step):              # –ò–¥—ë–º –ø–æ —Ç–µ–∫—Å—Ç—É —à–∞–≥–∞–º–∏
        piece = text[start:start + chunk_size]           # –ë–µ—Ä—ë–º –ø–æ–¥—Å—Ç—Ä–æ–∫—É –Ω—É–∂–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞
        if len(piece) < 50:                              # –°–æ–≤—Å–µ–º –∫—Ä–æ—à–µ—á–Ω—ã–µ –∫—É—Å–æ—á–∫–∏ –Ω–µ –±–µ—Ä—ë–º
            break
        chunks.append(piece)                             # –î–æ–±–∞–≤–ª—è–µ–º —á–∞–Ω–∫ –≤ —Å–ø–∏—Å–æ–∫
        if start + chunk_size >= len(text):              # –ï—Å–ª–∏ —ç—Ç–æ –±—ã–ª –ø–æ—Å–ª–µ–¥–Ω–∏–π —á–∞–Ω–∫ ‚Äî –≤—ã—Ö–æ–¥–∏–º
            break
    return chunks                                        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Å–ø–∏—Å–æ–∫ —á–∞–Ω–∫–æ–≤

def ensure_collection(client: QdrantClient, vector_size: int):
    """–°–æ–∑–¥–∞—ë–º –∫–æ–ª–ª–µ–∫—Ü–∏—é –≤ Qdrant Cloud, –µ—Å–ª–∏ –µ—ë –Ω–µ—Ç –∏–ª–∏ –ø–µ—Ä–µ—Å–æ–∑–¥–∞—ë–º –ø—Ä–∏ –∏–∑–º–µ–Ω–µ–Ω–∏–∏ —Ä–∞–∑–º–µ—Ä–∞ –≤–µ–∫—Ç–æ—Ä–∞."""
    if not client.collection_exists(COLLECTION_NAME):
        print(f"[INFO] –ö–æ–ª–ª–µ–∫—Ü–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞, —Å–æ–∑–¥–∞—ë–º –Ω–æ–≤—É—é '{COLLECTION_NAME}'...")
        client.create_collection(
            collection_name=COLLECTION_NAME,
            vectors_config=VectorParams(size=vector_size, distance=Distance.COSINE)
        )
    else:
        print(f"[INFO] –ö–æ–ª–ª–µ–∫—Ü–∏—è '{COLLECTION_NAME}' —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç.")

def get_gspread_client() -> gspread.Client:
    """–ê–≤—Ç–æ—Ä–∏–∑—É–µ–º—Å—è –≤ Google —Å –ø–æ–º–æ—â—å—é —Å–µ—Ä–≤–∏—Å–Ω–æ–≥–æ –∞–∫–∫–∞—É–Ω—Ç–∞ –∏ –ø–æ–ª—É—á–∞–µ–º –∫–ª–∏–µ–Ω—Ç gspread."""
    scopes = [
        "https://www.googleapis.com/auth/spreadsheets.readonly",
        "https://www.googleapis.com/auth/drive.readonly"
    ]
    creds = Credentials.from_service_account_file(
        SERVICE_ACCOUNT_FILE,
        scopes=scopes
    )
    return gspread.authorize(creds)

# --- –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –ø—Ä—è–º–æ–π —Å—Å—ã–ª–∫–∏ –Ω–∞ —Ñ–∞–π–ª Google Drive ---
def get_drive_image_url(file_id: str) -> str:
    # TODO: –í—Å—Ç–∞–≤—å—Ç–µ —Å–≤–æ–π Google Drive API –∫–ª—é—á/–ª–æ–≥–∏–Ω, –µ—Å–ª–∏ —Ç—Ä–µ–±—É–µ—Ç—Å—è –¥–æ—Å—Ç—É–ø –∫ –ø—Ä–∏–≤–∞—Ç–Ω—ã–º —Ñ–∞–π–ª–∞–º
    # –î–ª—è –ø—É–±–ª–∏—á–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Ç–∞–∫–æ–≥–æ —Ñ–æ—Ä–º–∞—Ç–∞:
    return f"https://drive.google.com/uc?id={file_id}"

def normalize_row(row: Dict[str, Any], mapping: Dict[str, Any], sheet_name: str) -> Dict[str, Any]:
    """
    –ü—Ä–∏–≤–æ–¥–∏–º —Å—Ç—Ä–æ–∫—É –ª–∏—Å—Ç–∞ –∫ ¬´—É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–æ–º—É¬ª –≤–∏–¥—É: title/text/url/image_url/category/subcategory/...
    row ‚Äî —Å–ª–æ–≤–∞—Ä—å –≤–∏–¥–∞ {–ò–º—è–ö–æ–ª–æ–Ω–∫–∏: –ó–Ω–∞—á–µ–Ω–∏–µ}; mapping ‚Äî –ø—Ä–∞–≤–∏–ª–∞ –¥–ª—è —Ç–µ–∫—É—â–µ–≥–æ –ª–∏—Å—Ç–∞; sheet_name ‚Äî –∏–º—è –ª–∏—Å—Ç–∞.
    """
    # –î–æ—Å—Ç–∞—ë–º –Ω–∞–∑–≤–∞–Ω–∏—è –∫–æ–ª–æ–Ω–æ–∫ –∏–∑ mapping (–≥–¥–µ —á—Ç–æ –ª–µ–∂–∏—Ç)
    title_col = mapping.get("title_col")                  # –ö–æ–ª–æ–Ω–∫–∞ —Å –∑–∞–≥–æ–ª–æ–≤–∫–æ–º
    text_cols = mapping.get("text_cols", [])              # –°–ø–∏—Å–æ–∫ –∫–æ–ª–æ–Ω–æ–∫, –∫–æ—Ç–æ—Ä—ã–µ —Å–∫–ª–µ–∏–º –≤ –æ—Å–Ω–æ–≤–Ω–æ–π —Ç–µ–∫—Å—Ç
    url_col = mapping.get("url_col")                      # –ö–æ–ª–æ–Ω–∫–∞ —Å–æ —Å—Å—ã–ª–∫–æ–π
    image_col = mapping.get("image_col")                  # –ö–æ–ª–æ–Ω–∫–∞ —Å —Ñ–æ—Ç–æ (URL)
    branch_col = mapping.get("branch_col")                # –ö–æ–ª–æ–Ω–∫–∞ —Å —Ñ–∏–ª–∏–∞–ª–æ–º
    updated_col = mapping.get("updated_col")              # –ö–æ–ª–æ–Ω–∫–∞ —Å –¥–∞—Ç–æ–π –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è
    price_col = mapping.get("price_col")                  # –ö–æ–ª–æ–Ω–∫–∞ —Å —Ü–µ–Ω–æ–π
    allergens_col = mapping.get("allergens_col")          # –ö–æ–ª–æ–Ω–∫–∞ —Å –∞–ª–ª–µ—Ä–≥–µ–Ω–∞–º–∏
    subcategory_col = mapping.get("subcategory_col")      # –ö–æ–ª–æ–Ω–∫–∞ —Å –ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏–µ–π (–º–æ–∂–µ—Ç –±—ã—Ç—å None)

    # –°–æ–±–∏—Ä–∞–µ–º —Ç–µ–∫—Å—Ç –∏–∑ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –∫–æ–ª–æ–Ω–æ–∫ (–ø—É—Å—Ç—ã–µ –∏–≥–Ω–æ—Ä–∏—Ä—É–µ–º)
    text_parts = []                                       # –°—é–¥–∞ —Å–æ–±–µ—Ä—ë–º –∫—É—Å–æ—á–∫–∏ —Ç–µ–∫—Å—Ç–∞
    for col in text_cols:                                 # –ò–¥—ë–º –ø–æ –∫–æ–ª–æ–Ω–∫–∞–º, —É–∫–∞–∑–∞–Ω–Ω—ã–º –¥–ª—è —Ç–µ–∫—Å—Ç–∞
        val = row.get(col)                                # –ë–µ—Ä—ë–º –∑–Ω–∞—á–µ–Ω–∏–µ —è—á–µ–π–∫–∏
        if val and str(val).strip():                      # –ï—Å–ª–∏ –Ω–µ–ø—É—Å—Ç–æ–µ ‚Äî –¥–æ–±–∞–≤–ª—è–µ–º
            text_parts.append(str(val).strip())

    # –°–∫–ª–µ–∏–≤–∞–µ–º –≤ –æ–¥–∏–Ω —Ç–µ–∫—Å—Ç (—á–µ—Ä–µ–∑ –ø–µ—Ä–µ–≤–æ–¥ —Å—Ç—Ä–æ–∫–∏)
    main_text = "\n".join(text_parts) if text_parts else ""  # –ï—Å–ª–∏ –ø—É—Å—Ç–æ ‚Äî –ø—É—Å—Ç–∞—è —Å—Ç—Ä–æ–∫–∞

    # –§–æ—Ä–º–∏—Ä—É–µ–º ¬´—É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—É—é¬ª –∑–∞–ø–∏—Å—å (payload)
    payload = {
        "doc_id": f"{sheet_name}:{row.get(title_col, '')}",  # doc_id: –∏–º—è –ª–∏—Å—Ç–∞ + –∑–∞–≥–æ–ª–æ–≤–æ–∫ (–ø—Ä–æ—Å—Ç–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è)
        "category": mapping.get("category", sheet_name),      # –ö–∞—Ç–µ–≥–æ—Ä–∏—è: –∏–∑ mapping –∏–ª–∏ —Å–∞–º sheet_name
        "subcategory": row.get(subcategory_col, "") if subcategory_col else "",  # –ü–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏—è (–µ—Å–ª–∏ –µ—Å—Ç—å)
        "title": str(row.get(title_col, "")),                 # –ó–∞–≥–æ–ª–æ–≤–æ–∫
        "text": main_text,                                    # –û—Å–Ω–æ–≤–Ω–æ–π —Ç–µ–∫—Å—Ç (—Å–∫–ª–µ–π–∫–∞ –∫–æ–ª–æ–Ω–æ–∫)
        "url": str(row.get(url_col, "")) if url_col else "", # –°—Å—ã–ª–∫–∞
        "image_url": "",                                     # –§–æ—Ç–æ (URL)
        "branch_id": str(row.get(branch_col, "")) if branch_col else "",# –§–∏–ª–∏–∞–ª
        "updated_at": str(row.get(updated_col, "")) if updated_col else "",  # –î–∞—Ç–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è
        "price": str(row.get(price_col, "")) if price_col else "",           # –¶–µ–Ω–∞
        "allergens": str(row.get(allergens_col, "")) if allergens_col else ""# –ê–ª–ª–µ—Ä–≥–µ–Ω—ã
    }
    if image_col and row.get(image_col):
        val = str(row.get(image_col, "")).strip()
        if val.startswith("http"):
            payload["image_url"] = val
        else:  # –ï—Å–ª–∏ —ç—Ç–æ file_id –∏–∑ Google Drive
            payload["image_url"] = get_drive_image_url(val)
    return payload                                         # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –ø–æ–ª—è

def main():
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —ç–º–±–µ–¥–¥–µ—Ä (–ø—Ä–∏ –ø–µ—Ä–≤–æ–º –∑–∞–ø—É—Å–∫–µ –º–æ–¥–µ–ª—å —Å–∫–∞—á–∞–µ—Ç—Å—è)
    embedder = SentenceTransformer(EMBEDDING_MODEL_NAME)  # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤

    # –ü–æ–¥–∫–ª—é—á–∞–µ–º—Å—è –∫ Qdrant
    client = QdrantClient(
        url=QDRANT_HOST,
        api_key=QDRANT_API_KEY,
    )  # –°–æ–∑–¥–∞—ë–º –∫–ª–∏–µ–Ω—Ç Qdrant

    # –ì–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ–º, —á—Ç–æ –∫–æ–ª–ª–µ–∫—Ü–∏—è —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –∏ –∏–º–µ–µ—Ç –ø—Ä–∞–≤–∏–ª—å–Ω—É—é —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å
    vector_size = embedder.get_sentence_embedding_dimension()  # –ü–æ–ª—É—á–∞–µ–º —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–∞
    ensure_collection(client, vector_size)                     # –°–æ–∑–¥–∞—ë–º –∫–æ–ª–ª–µ–∫—Ü–∏—é, –µ—Å–ª–∏ –Ω–µ—Ç

    # –õ–æ–≥–∏–Ω–∏–º—Å—è –≤ Google –∏ –æ—Ç–∫—Ä—ã–≤–∞–µ–º —Ç–∞–±–ª–∏—Ü—É
    gc = get_gspread_client()                                  # –ê–≤—Ç–æ—Ä–∏–∑—É–µ–º—Å—è gspread-–∫–ª–∏–µ–Ω—Ç–æ–º
    sh = gc.open_by_key(SPREADSHEET_ID)                         # –û—Ç–∫—Ä—ã–≤–∞–µ–º —Ç–∞–±–ª–∏—Ü—É –ø–æ ID

    points_batch = []                                          # –°—é–¥–∞ –±—É–¥–µ–º —Å–∫–ª–∞–¥—ã–≤–∞—Ç—å —á–∞–Ω–∫–∏ –¥–ª—è –±–∞—Ç—á-–≤—Å—Ç–∞–≤–∫–∏
    total_chunks = 0                                           # –°—á—ë—Ç—á–∏–∫ —Å–æ–∑–¥–∞–Ω–Ω—ã—Ö —á–∞–Ω–∫–æ–≤

    # –ò–¥—ë–º –ø–æ –∫–∞–∂–¥–æ–º—É –ª–∏—Å—Ç—É —Å–æ–≥–ª–∞—Å–Ω–æ –Ω–∞—à–µ–π –∫–∞—Ä—Ç–µ SHEETS_MAPPING
    for sheet_name, mapping in SHEETS_CONFIG.items():         # –ü–µ—Ä–µ–±–∏—Ä–∞–µ–º –ø–∞—Ä—ã ¬´–∏–º—è –ª–∏—Å—Ç–∞¬ª ‚Üí ¬´–ø—Ä–∞–≤–∏–ª–∞¬ª
        try:
            ws = sh.worksheet(sheet_name)                      # –û—Ç–∫—Ä—ã–≤–∞–µ–º –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π –ª–∏—Å—Ç –ø–æ –∏–º–µ–Ω–∏
        except Exception as e:
            print(f"[WARN] –õ–∏—Å—Ç '{sheet_name}' –Ω–µ –Ω–∞–π–¥–µ–Ω: {e}")# –ï—Å–ª–∏ –ª–∏—Å—Ç–∞ –Ω–µ—Ç ‚Äî –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–∞–µ–º –∏ –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º
            continue

        rows = ws.get_all_records()                            # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ —Å—Ç—Ä–æ–∫–∏ –∫–∞–∫ —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π {–ö–æ–ª–æ–Ω–∫–∞: –ó–Ω–∞—á–µ–Ω–∏–µ}
        if not rows:                                           # –ï—Å–ª–∏ –ª–∏—Å—Ç –ø—É—Å—Ç–æ–π
            print(f"[INFO] –õ–∏—Å—Ç '{sheet_name}' –ø—É—Å—Ç.")
            continue

        for row in tqdm(rows, desc=f"–ß–∏—Ç–∞–µ–º –ª–∏—Å—Ç '{sheet_name}'"):  # –ü—Ä–æ–≥—Ä–µ—Å—Å –ø–æ —Å—Ç—Ä–æ–∫–∞–º –ª–∏—Å—Ç–∞
            # –§–æ—Ä–º–∏—Ä—É–µ–º payload –∏–∑ –≤—Å–µ—Ö –Ω—É–∂–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫ FAQ
            payload = {
                "question": row.get("question"),
                "anwser": row.get("anwser"),
                "category": row.get("category"),
                "tags": row.get("tags"),
                "sourse": row.get("sourse"),
                "image_url": row.get("image_url"),
                "last_updated": row.get("last_updated")
            }
            question = str(payload.get("question") or "")
            anwser = str(payload.get("anwser") or "")
            if not anwser and not question:
                continue
            # –í–µ–∫—Ç–æ—Ä —Å—Ç—Ä–æ–∏–º –ø–æ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—é question + anwser
            text_for_embedding = (question + " " + anwser).strip()
            if not text_for_embedding:
                continue
            vec = embedder.encode(text_for_embedding).astype(np.float32)
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–π id –∏–∑ —Ç–∞–±–ª–∏—Ü—ã, –µ—Å–ª–∏ –æ–Ω –µ—Å—Ç—å, –∏–Ω–∞—á–µ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º UUID
            # –í—Å–µ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑—É–µ–º UUID –¥–ª—è id
            pid = str(uuid.uuid4())
            point_payload = dict(payload)
            point = PointStruct(
                id=pid,
                vector=vec,
                payload=point_payload
            )
            points_batch.append(point)
            total_chunks += 1
            if len(points_batch) >= 500:
                print("POINTS TO UPSERT:", points_batch)
                client.upsert(collection_name=COLLECTION_NAME, points=points_batch)
                points_batch = []

    # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º ¬´—Ö–≤–æ—Å—Ç¬ª (–æ—Å—Ç–∞—Ç–∫–∏ –º–µ–Ω–µ–µ 500)
    if points_batch:
        client.upsert(collection_name=COLLECTION_NAME, points=points_batch)  # –§–∏–Ω–∞–ª—å–Ω–∞—è –∑–∞–ø–∏—Å—å
    print(f"[INFO] –ó–∞–≥—Ä—É–∂–µ–Ω–æ {total_chunks} —á–∞–Ω–∫–æ–≤ –≤ Qdrant Cloud.")
    if points_batch:
        client.upsert(collection_name=COLLECTION_NAME, points=points_batch)  # –§–∏–Ω–∞–ª—å–Ω–∞—è –∑–∞–ø–∏—Å—å

    print(f"[DONE] –ó–∞–≥—Ä—É–∂–µ–Ω–æ —á–∞–Ω–∫–æ–≤: {total_chunks}")           # –û—Ç—á—ë—Ç –æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö —á–∞–Ω–∫–æ–≤

if __name__ == "__main__":
    main()                                                      # –¢–æ—á–∫–∞ –≤—Ö–æ–¥–∞
